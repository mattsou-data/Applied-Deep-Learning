\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{setspace}

\renewcommand{\baselinestretch}{1.2}

% Title and Headers
\title{\textbf{ReadMe}}
\author{}
\date{}

\begin{document}

\maketitle

\noindent\textbf{(The work breakdown is present in Section \ref{sec:work_breakdown}, and the error metric is specified in Section \ref{sec:discussions}.)}

\section*{Presentation of the Project}

The goal of the project is to successfully measure the ``sentiment'' towards Bitcoin and the cryptocurrency ecosystem and use it to identify good zones for selling or buying (tops or bottoms) Bitcoin. It will require us to collect data, specifically YouTube comments, and correctly label them as positive or negative. The percentage of positive comments and the number of comments each day will provide us with information to create our own indicators.

\section{Run the Project}
\subsection*{Part 1: Setting Up the Environment}
\begin{enumerate}[label=Step \arabic*:]
    \item Run the Python script, \texttt{setup\_script.py}, in a terminal with the command:
    \begin{verbatim}
    python setup_script.py
    \end{verbatim}
    It will automatically create a virtual environment called `venv` and install the dependencies inside the \texttt{requirements.txt} file.
\end{enumerate}

\subsection*{Part 2: Import All the Necessary Datasets}
\begin{enumerate}[label=Step \arabic*:, start=2]
    \item Download the daily information about Bitcoin for the last years by running the \texttt{Import\_bitcoin\_data.ipynb} notebook. We use the `yfinance` library, and the data is saved in a CSV file called \texttt{bitcoin\_data.csv}.
    \item Download all the YouTube comments under the daily videos from the channel \textit{Le Trone Crypto}. Run the notebook \texttt{Pull\_all\_Youtube\_comments.ipynb}. Note that it will require a YouTube API key. It will store:
    \begin{itemize}
        \item All video IDs in the file \texttt{video\_ids.csv}.
        \item All retrieved comments in \texttt{comments\_data/combined\_data.csv}.
    \end{itemize}
    \item Download a labeled dataset of French tweets from Kaggle to fine-tune our model. Run the \texttt{Twitter\_extract.ipynb} notebook (a \texttt{kaggle.json} file is required). The data will be saved in \texttt{french\_tweets.csv}.
\end{enumerate}

\subsection*{Part 3: Download and Fine-Tune the Model}
\begin{enumerate}[label=Step \arabic*:, start=5]
    \item Run the Python script \texttt{Tweets\_sampling.py} to randomly select 10,000 tweets from \texttt{french\_tweets.csv} and save them in \texttt{sampled\_tweets.csv}.
    \item Use the CamemBERT model from HuggingFace for labeling YouTube comments. Import and fine-tune it using \texttt{Fine-tunning.ipynb}.
\end{enumerate}

\subsection*{Part 4: Label Comments and Create Visualizations}
\begin{enumerate}[label=Step \arabic*:, start=7]
    \item Run \texttt{Labeling\_yb\_comments.ipynb}. Note: the process takes around 75 minutes. The labeled data will be saved in \texttt{comments\_data/labeled\_data.csv}.
    \item Run \texttt{First\_visualisations.ipynb} to analyze the results, find correlations, and build the indicator.
\end{enumerate}

\section{Work Breakdown} \label{sec:work_breakdown}
\begin{itemize}
    \item \textbf{Collecting Data:}
    \begin{itemize}
        \item Choice of data: 10 hours
        \item YouTube comments (API): 10 hours
        \item Tweets (Kaggle): 2 hours
        \item Bitcoin data (Yfinance): 30 minutes
    \end{itemize}
    \item \textbf{Model and Training:}
    \begin{itemize}
        \item Constructing my own model: 15 hours
        \item Preparing data: 5 hours
        \item Importing HuggingFace model: 3 hours
        \item Training: 12 hours
    \end{itemize}
    \item \textbf{Labeling Data and Visualizations:}
    \begin{itemize}
        \item Preparing data: 4 hours
        \item Analyzing results: 8 hours
    \end{itemize}
    \item \textbf{Creating the Indicator:} 6 hours
\end{itemize}

\section{Discussions} \label{sec:discussions}

\subsection*{Model and Labeling Quality}
The error metric is the accuracy of the predicted label compared to manual labeling. I aimed for 90\% accuracy but achieved 86\% using a custom model. Switching to CamemBERT improved the accuracy to 97\% after fine-tuning.

\subsection*{YouTube Comments: Pros and Cons}
\textbf{Pros:}
\begin{enumerate}
    \item Large historical data since 2019.
    \item Daily videos reflect timely sentiment.
    \item Consistent format and community.
\end{enumerate}

\textbf{Cons/Challenges:}
\begin{enumerate}
    \item Comment volume grows over time.
    \item Positive bias due to community behavior.
    \item Inconsistent posting during vacations.
\end{enumerate}

\subsection*{Choice of Twitter Data for Fine-Tuning}
Tweets and YouTube comments are similar enough for classification tasks, and Twitter datasets are more accessible.

\subsection*{Insights That Helped Create the Indicator}
\begin{enumerate}
    \item Most people will lose in the market as it is a zero-sum game. Therefore, the biggest movements will always go against the dominant opinion. The definitive bottoms will be found in periods of fear and disinterest, while the major tops of markets occur when people are euphoric, and many get trapped in the market.
    \item The number of comments and the percentage of positive comments are necessary and sufficient to build a reliable indicator. One translates the global interest in the market, and the other reflects the sentiment towards it.
    \item Those sentiments are very volatile and won't be so reliable in the short term (daily analysis). Instead, we should focus on using them to predict trends at a larger time scale (weekly, monthly, or annually).
\end{enumerate}

\subsection*{Performances of the Indicator}
If we were to develop a strategy where:
\begin{itemize}
    \item Buy when the indicator score is under -25 and the 7-day average is below the 30-day average,
    \item Sell when the indicator goes over 50 and the 7-day average exceeds the 30-day average,
\end{itemize}
we would have:

\subsubsection*{BOUGHT at the PRICES}
\begin{verbatim}
2022-08-02    22978.117188
2022-09-25    18802.097656
2022-11-12    16799.185547
2022-12-27    16717.173828
2022-12-31    16547.496094
2023-06-06    27238.783203
2023-10-01    27983.750000
\end{verbatim}

\subsubsection*{SOLD at the PRICES}
\begin{verbatim}
2021-11-14    65466.839844
2021-11-17    60368.011719
2023-11-18    36585.703125
\end{verbatim}

\noindent We identify that:
\begin{itemize}
    \item The indicator perfectly captured the bottom of the market without sending false buy signals at the wrong time.
    \item The mean entry price is very low, offering a potential 500\% gain with the current Bitcoin price.
    \item The indicator also perfectly identified the exact top of the market in the previous cycle. However, it gave a false signal of a top in November 2023.
\end{itemize}

\noindent The proposed strategy is very simple and should incorporate other elements, such as price action, before making a financial decision.

\subsubsection*{Limits}
\begin{itemize}
    \item The indicator has only one cycle worth of data, and we still need confirmation to verify if it is well-tuned for predictions in the new cycle.
    \item It can give false signals when the market is going up.
    \item The results given before the beginning of the year 2020 are unreliable because the number of comments was too low.
\end{itemize}
\section*{Further Integration}
In the future, I aim to integrate the indicator into TradingView and automate daily data collection.

\end{document}

